# ä»…ä½¿ç”¨`numpy`ç¼–å†™çš„æ·±åº¦å­¦ä¹ æ¡†æ¶

- åœ¨æ¡†æ¶ä¸ŠåŸºæœ¬æ¨¡ä»¿`pytorch`ï¼Œç”¨ä»¥å­¦ä¹ ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç®—æ³•ï¼Œå¦‚å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€å„ç§å±‚ã€å„ç§æ¿€æ´»å‡½æ•°
- é‡‡ç”¨é¢å‘å¯¹è±¡çš„æ€æƒ³è¿›è¡Œç¼–ç¨‹ï¼Œæ€è·¯è¾ƒä¸ºæ¸…æ™°
- æƒ³è¦è‡ªå·±**æ‰‹å†™ç¥ç»ç½‘ç»œ**çš„åŒå­¦ä»¬å¯ä»¥å‚è€ƒä¸€ä¸‹
- ä»£ç å¤§ä½“æ¡†æ¶è¾ƒä¸ºæ¸…æ™°ï¼Œä½†ä¸å¦è®¤å­˜åœ¨ä¸‘é™‹çš„éƒ¨åˆ†ï¼Œä»¥åŠå¯¹äº`pytorch`çš„æ‹™åŠ£æ¨¡ä»¿

## é¡¹ç›®ä»‹ç»

- ### `MINST_recognition`:
		
	- æ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œä½¿ç”¨`MINST`æ•°æ®é›†
	
	- è®­ç»ƒ30è½®å¯ä»¥è¾¾åˆ°93%å‡†ç¡®åº¦ï¼Œè®­ç»ƒ500è½®å·¦å³è¾¾åˆ°95%å‡†ç¡®åº¦æ— æ³•ç»§ç»­ä¸Šå‡
	
- ### `RNN_sin_to_cos`:

	- ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œRNNï¼Œç”¨$sin$çš„æ›²çº¿é¢„æµ‹$cos$çš„æ›²çº¿
	
	- ç›®å‰ä»æœ‰bugï¼Œæ— æ³•æ­£å¸¸è®­ç»ƒ

## æ¡†æ¶ä»‹ç»

- ä¸æ¡†æ¶æœ‰å…³çš„ä»£ç éƒ½æ”¾åœ¨äº†`mtorch`æ–‡ä»¶å¤¹ä¸­

- ### ä½¿ç”¨æµç¨‹

  - ä¸`pytorch`ç›¸ä¼¼ï¼Œéœ€è¦å®šä¹‰è‡ªå·±çš„ç¥ç»ç½‘ç»œã€æŸå¤±å‡½æ•°ã€æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–ç®—æ³•ç­‰ç­‰

  - åœ¨æ¯ä¸€è½®çš„è®­ç»ƒä¸­ï¼Œå…ˆè·å–æ ·æœ¬è¾“å…¥å°†å…¶è¾“å…¥åˆ°è‡ªå·±çš„ç¥ç»ç½‘ç»œä¸­è·å–è¾“å‡ºã€‚ç„¶åå°†**é¢„æµ‹ç»“æœå’ŒæœŸæœ›ç»“æœ**äº¤ç»™æŸå¤±å‡½æ•°è®¡ç®—`loss`ï¼Œå¹¶é€šè¿‡`loss`è¿›è¡Œæ¢¯åº¦çš„è®¡ç®—ï¼Œæœ€åé€šè¿‡ä¼˜åŒ–å™¨å¯¹ç¥ç»ç½‘ç»œçš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚

  - ç»“åˆä»£ç ç†è§£æ›´ä½³ğŸ‘‡ï¼š

  - ä»¥ä¸‹æ˜¯ä½¿ç”¨`MINST`æ•°æ®é›†çš„**æ‰‹å†™æ•°å­—è¯†åˆ«çš„ä¸»ä½“ä»£ç **

  - ```python
  	# å®šä¹‰ç½‘ç»œ define neural network
  	class DigitModule(Module):
  	    def __init__(self):
  	        # è®¡ç®—é¡ºåºå°±ä¼šæŒ‰ç…§è¿™é‡Œå®šä¹‰çš„é¡ºåºè¿›è¡Œ
  	        sequential = Sequential([
  	            layers.Linear2(in_dim=ROW_NUM * COLUM_NUM, out_dim=16, coe=2),
  	            layers.Relu(16),
  	            layers.Linear2(in_dim=16, out_dim=16, coe=2),
  	            layers.Relu(16),
  	            layers.Linear2(in_dim=16, out_dim=CLASS_NUM, coe=1),
  	            layers.Sigmoid(CLASS_NUM)
  	        ])
  	        super(DigitModule, self).__init__(sequential)
  	
  	
  	module = DigitModule()  # åˆ›å»ºæ¨¡å‹ create module
  	loss_func = SquareLoss(backward_func=module.backward)  # å®šä¹‰æŸå¤±å‡½æ•° define loss function
  	optimizer = SGD(module, lr=learning_rate)  # å®šä¹‰ä¼˜åŒ–å™¨ define optimizer
  	
  	
  	for i in range(EPOCH_NUM):  # å…±è®­ç»ƒEPOCH_NUMè½®
  	    trainning_loss = 0  # è®¡ç®—ä¸€ä¸‹å½“å‰ä¸€è½®è®­ç»ƒçš„losså€¼ï¼Œå¯ä»¥æ²¡æœ‰
  	    for data in train_loader:  # éå†æ‰€æœ‰æ ·æœ¬ï¼Œtrain_loaderæ˜¯å¯è¿­ä»£å¯¹è±¡ï¼Œä¿å­˜äº†æ•°æ®é›†ä¸­æ‰€æœ‰çš„æ•°æ®
  	        imgs, targets = data  # å°†æ•°æ®æ‹†åˆ†æˆå›¾ç‰‡å’Œæ ‡ç­¾
  	        outputs = module(imgs)  # å°†æ ·æœ¬çš„è¾“å…¥å€¼è¾“å…¥åˆ°è‡ªå·±çš„ç¥ç»ç½‘ç»œä¸­
  	        loss = loss_func(outputs, targets, transform=True)  # è®¡ç®—loss / calculate loss
  	        trainning_loss += loss.value
  	        loss.backward()  # é€šè¿‡åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ / calculate gradiant through back propagation
  	        optimizer.step()  # é€šè¿‡ä¼˜åŒ–å™¨è°ƒæ•´æ¨¡å‹å‚æ•° / adjust the weights of network through optimizer
  	    if i % TEST_STEP == 0:  # æ¯è®­ç»ƒTEST_STEPè½®å°±æµ‹è¯•ä¸€ä¸‹å½“å‰è®­ç»ƒçš„æˆæœ
  	        show_effect(i, module, loss_func, test_loader, i // TEST_STEP)
  	        print("{} turn finished, loss of train set = {}".format(i, trainning_loss))
  	```

- æ¥ä¸‹æ¥é€ä¸ªä»‹ç»ç¼–å†™çš„ç±»ï¼Œè¿™äº›ç±»åœ¨`pytorch`ä¸­éƒ½æœ‰åŒååŒåŠŸèƒ½çš„ç±»ï¼Œæ˜¯ä»¿ç…§`pytorch`æ¥çš„ï¼š

- ### `Module`ç±»

	- ä¸`pytorch`ä¸åŒï¼Œåªèƒ½æœ‰ä¸€ä¸ª`Sequential`ç±»ï¼ˆåºåˆ—ï¼‰ï¼Œåœ¨è¯¥ç±»ä¸­å®šä¹‰å¥½ç¥ç»ç½‘ç»œçš„å„ä¸ªå±‚å’Œé¡ºåºï¼Œç„¶åä¼ ç»™`Module`ç±»çš„æ„é€ å‡½æ•°
	- **æ­£å‘ä¼ æ’­ï¼š**è°ƒç”¨`Sequential`çš„æ­£å‘ä¼ æ’­
	- **åå‘ä¼ æ’­ï¼š**è°ƒç”¨`Sequential`çš„åå‘ä¼ æ’­
	- ç›®å‰ä¸ºæ­¢ï¼Œè¿™ä¸ªç±»çš„å¤§éƒ¨åˆ†åŠŸèƒ½ä¸`Sequential`ç›¸åŒï¼Œåªæ˜¯**å¥—äº†ä¸ªå£³**ä¿è¯ä¸`pytorch`ç›¸åŒ

- ### `lossfunction`

	- æœ‰ä¸åŒçš„`loss`å‡½æ•°ï¼Œæ„é€ å‡½æ•°éœ€è¦ç»™ä»–æŒ‡å®š**è‡ªå·±å®šä¹‰çš„ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­å‡½æ•°**
	- è°ƒç”¨`loss`å‡½æ•°ä¼šè¿”å›ä¸€ä¸ª`Loss`ç±»çš„å¯¹è±¡ï¼Œè¯¥ç±»è®°å½•äº†`loss`å€¼ã€‚
	- é€šè¿‡è°ƒç”¨`Loss`ç±»çš„`.backward()`æ–¹æ³•å°±å¯ä»¥å®ç°åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
	- å†…éƒ¨æœºåˆ¶ï¼š
		- å†…éƒ¨å…¶å®å°±æ˜¯è°ƒç”¨äº†**è‡ªå·±å®šä¹‰çš„ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­å‡½æ•°**
		- ä¹Ÿç®—æ˜¯å¯¹äº`pytorch`çš„ä¸€ä¸ª**æ‹™åŠ£æ¨¡ä»¿ï¼Œå®Œå…¨æ²¡å¿…è¦**ï¼Œç›´æ¥é€šè¿‡`Module`è°ƒç”¨å°±å¥½

- ### ä¼˜åŒ–å™¨ï¼š

  - ç›®å‰åªå®ç°äº†**éšæœºæ¢¯åº¦ä¸‹é™SGD**
  - æ„é€ å‡½æ•°çš„å‚æ•°æ˜¯**è‡ªå·±å®šä¹‰çš„`Module`**ã€‚åœ¨å·²ç»è®¡ç®—è¿‡æ¢¯åº¦ä¹‹åï¼Œè°ƒç”¨`optimizer.step()`æ”¹å˜`Module`å†…å„ä¸ªå±‚çš„å‚æ•°å€¼
  - å†…éƒ¨æœºåˆ¶ï¼š
    - ç›®å‰ç”±äºåªæœ‰SGDä¸€ç§ç®—æ³•ï¼Œæ‰€ä»¥æš‚æ—¶ä¹Ÿåªæ˜¯ä¸€ä¸ª**æ‹™åŠ£æ¨¡ä»¿**
    - å°±æ˜¯è°ƒç”¨äº†ä¸€ä¸‹`Module.step()`ï¼Œå†è®©`Module`è°ƒç”¨`Sequential.step()`ï¼Œæœ€åç”±`Sequential`è°ƒç”¨å†…éƒ¨å„ä¸ªå±‚çš„`Layer.step()`å®ç°æ›´æ–°
    - æ¢¯åº¦å€¼åœ¨`loss.backward`çš„æ—¶å€™è®¡ç®—ã€ä¿å­˜åœ¨å„ä¸ªå±‚ä¸­äº†

- ### `Layer`ç±»

	- æœ‰è®¸å¤šä¸åŒçš„å±‚

	- #### å…±æ€§

		- **å‰å‘ä¼ æ’­**ï¼š
			- æ¥å—ä¸€ä¸ªè¾“å…¥è¿›è¡Œå‰å‘ä¼ æ’­è®¡ç®—ï¼Œè¾“å‡ºä¸€ä¸ªè¾“å‡º
			- ä¼šå°†è¾“å…¥ä¿å­˜èµ·æ¥ï¼Œåœ¨åå‘ä¼ æ’­ä¸­è¦ç”¨
		- **åå‘ä¼ æ’­**ï¼š
			- æ¥å—**å‰å‘ä¼ æ’­çš„è¾“å‡ºçš„æ¢¯åº¦å€¼**ï¼Œè®¡ç®—**è‡ªèº«å‚æ•°ï¼ˆå¦‚Linearä¸­çš„wå’Œbï¼‰çš„æ¢¯åº¦å€¼**å¹¶ä¿å­˜èµ·æ¥
			- è¾“å‡ºå€¼ä¸º**å‰å‘ä¼ æ’­çš„è¾“å…¥çš„æ¢¯åº¦å€¼**ï¼Œç”¨æ¥è®©ä¸Šä¸€å±‚ï¼ˆå¯èƒ½æ²¡æœ‰ï¼‰ç»§ç»­è¿›è¡Œåå‘ä¼ æ’­è®¡ç®—
			- è¿™æ ·ä¸åŒçš„å±‚ä¹‹é—´å°±å¯ä»¥è¿›è¡Œä»»æ„çš„æ‹¼è£…è€Œä¸å¦¨ç¢å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­çš„è¿›è¡Œäº†
		- **`.step`æ–¹æ³•**
			- æ›´æ–°è‡ªèº«çš„å‚æ•°å€¼ï¼ˆä¹Ÿå¯èƒ½æ²¡æœ‰ï¼Œå¦‚æ¿€æ´»å±‚ã€æ± åŒ–å±‚ï¼‰

	- #### `Sequential`ç±»

		- è¿™ä¸ªç±»ä¹Ÿæ˜¯ç»§æ‰¿è‡ª`Layer`ï¼Œå¯ä»¥å½“ä½œä¸€å±‚æ¥ä½¿ç”¨

		- å®ƒæŠŠå¤šä¸ªå±‚æŒ‰ç…§é¡ºåºæ‹¼è£…åˆ°ä¸€èµ·ï¼Œåœ¨å‰å‘ã€åå‘ä¼ æ’­æ—¶æŒ‰ç…§é¡ºåºè¿›è¡Œè®¡ç®—

		- ç»“åˆå®ƒçš„`forward`ã€`backward`æ–¹æ³•æ¥ç†è§£ï¼š

			- ```python
				def forward(self, x):
				    out = x
				    for layer in self.layers:
				        out = layer(out)
				    return out
				
				def backward(self, output_gradient):
				    layer_num = len(self.layers)
				    delta = output_gradient
				    for i in range(layer_num - 1, -1, -1):
				        # åå‘éå†å„ä¸ªå±‚, å°†æœŸæœ›æ”¹å˜é‡åå‘ä¼ æ’­
				        delta = self.layers[i].backward(delta)
				
				def step(self, lr):
				    for layer in self.layers:
				        layer.step(lr)
				```

			

	- ### `RNN`ç±»ï¼šå¾ªç¯ç¥ç»ç½‘ç»œå±‚

		- ç»§æ‰¿è‡ª`Layer`ï¼Œç”±äºå†…å®¹æ¯”è¾ƒå¤æ‚æ•…å•ç‹¬è¯´æ˜ä¸€ä¸‹

		- `RNN`å†…éƒ¨ç”±ä¸€ä¸ª**å…¨è¿æ¥å±‚`Linear`**å’Œä¸€ä¸ª**æ¿€æ´»å±‚**ç»„æˆ

		- #### å‰å‘ä¼ æ’­

			- ```python
				    def forward(self, inputs):
				        """
				        :param inputs: input = (h0, x) h0.shape == (batch, out_dim) x.shape == (seq, batch, in_dim)
				        :return: outputs: outputs.shape == (seq, batch, out_dim)
				        """
				        h = inputs[0]  # è¾“å…¥çš„inputsç”±ä¸¤éƒ¨åˆ†ç»„æˆ
				        X = inputs[1]
				        if X.shape[2] != self.in_dim or h.shape[1] != self.out_dim:
				            # æ£€æŸ¥è¾“å…¥çš„å½¢çŠ¶æ˜¯å¦æœ‰é—®é¢˜
				            raise ShapeNotMatchException(self, "forward: wrong shape: h0 = {}, X = {}".format(h.shape, X.shape))
				
				        self.seq_len = X.shape[0]  # æ—¶é—´åºåˆ—çš„é•¿åº¦
				        self.inputs = X  # ä¿å­˜è¾“å…¥ï¼Œä¹‹åçš„åå‘ä¼ æ’­è¿˜è¦ç”¨
				        output_list = []  # ä¿å­˜æ¯ä¸ªæ—¶é—´ç‚¹çš„è¾“å‡º
				        for x in X:
				            # æŒ‰æ—¶é—´åºåˆ—éå†input
				            # x.shape == (batch, in_dim), h.shape == (batch, out_dim)
				            h = self.activation(self.linear(np.c_[h, x]))
				            output_list.append(h)
				        self.outputs = np.stack(output_list, axis=0)  # å°†åˆ—è¡¨è½¬æ¢æˆä¸€ä¸ªçŸ©é˜µä¿å­˜èµ·æ¥
				        return self.outputs
				```

		- #### åå‘ä¼ æ’­

			- ```python
				def backward(self, output_gradient):
				    """
				    :param output_gradient: shape == (seq, batch, out_dim)
				    :return: input_gradiant
				    """
				    if output_gradient.shape != self.outputs.shape:
				        # æœŸæœ›å¾—åˆ°(seq, batch, out_dim)å½¢çŠ¶
				        raise ShapeNotMatchException(self, "__backward: expected {}, but we got "
				                                           "{}".format(self.outputs.shape, output_gradient.shape))
				
				    input_gradients = []
				    # æ¯ä¸ªtime_stepä¸Šçš„è™šæ‹Ÿweight_gradient, æœ€åæ±‚å¹³å‡å€¼å°±æ˜¯æ€»çš„weight_gradient
				    weight_gradients = np.zeros(self.linear.weights_shape())
				    bias_gradients = np.zeros(self.linear.bias_shape())
				    batch_size = output_gradient.shape[1]
				
				    # total_gradient: å‰å‘ä¼ æ’­çš„æ—¶å€™æ˜¯å°†x, håˆæˆä¸ºä¸€ä¸ªçŸ©é˜µï¼Œæ‰€ä»¥åå‘ä¼ æ’­ä¹Ÿå…ˆè®¡ç®—è¿™ä¸ªå¤§çŸ©é˜µçš„æ¢¯åº¦å†æ‹†åˆ†ä¸ºx_grad, h_grad
				    total_gradient = np.zeros((batch_size, self.out_dim + self.in_dim))
				    h_gradient = None
				    
				    # åå‘éå†å„ä¸ªæ—¶é—´å±‚ï¼Œè®¡ç®—è¯¥å±‚çš„æ¢¯åº¦å€¼
				    for i in range(self.seq_len - 1, -1, -1):
				        # å‰å‘ä¼ æ’­é¡ºåº: x, h -> z -> h
				        # æ‰€ä»¥åå‘ä¼ æ’­è®¡ç®—é¡ºåºï¼šh_grad -> z_grad -> x_grad, h_grad, w_grad, b_grad
				
				        # %%%%%%%%%%%%%%è®¡ç®—å¹³å‡å€¼çš„ç‰ˆæœ¬%%%%%%%%%%%%%%%%%%%%%%%
				        # h_gradient = (output_gradient[i] + total_gradient[:, 0:self.out_dim]) / 2
				        # %%%%%%%%%%%%%%ä¸è®¡ç®—å¹³å‡å€¼çš„ç‰ˆæœ¬%%%%%%%%%%%%%%%%%%%%%%%
				        #  è®¡ç®—h_grad: è¿™ä¸€æ—¶é—´ç‚¹çš„h_gradåŒ…æ‹¬è¾“å‡ºçš„gradå’Œä¹‹å‰çš„æ—¶é—´ç‚¹è®¡ç®—æ‰€å¾—gradä¸¤éƒ¨åˆ†
				        h_gradient = output_gradient[i] + total_gradient[:, 0:self.out_dim]  
				
				        # w_gradå’Œb_gradæ˜¯åœ¨linear.backward()å†…è®¡ç®—çš„ï¼Œä¸ç”¨æ‰‹åŠ¨å†è®¡ç®—äº†
				        z_gradient = self.activation.backward(h_gradient)  # è®¡ç®—z_grad
				        total_gradient = self.linear.backward(z_gradient)  # è®¡ç®—x_gradå’Œh_gradåˆæˆçš„å¤§çŸ©é˜µçš„æ¢¯åº¦
				
				        # total_gradient åŒæ—¶åŒ…å«äº†hå’Œxçš„gradient, shape == (batch, out_dim + in_dim)
				        x_gradient = total_gradient[:, self.out_dim:]
				
				        input_gradients.append(x_gradient)  
				        weight_gradients += self.linear.gradients["w"]
				        bias_gradients += self.linear.gradients["b"]
				
				    # %%%%%%%%%%%%%%%%%%è®¡ç®—å¹³å‡å€¼çš„ç‰ˆæœ¬%%%%%%%%%%%%%%%%%%%%%%%
				    # self.linear.set_gradients(w=weight_gradients / self.seq_len, b=bias_gradients / self.seq_len)
				    # %%%%%%%%%%%%%%%%%%ä¸è®¡ç®—å¹³å‡å€¼çš„ç‰ˆæœ¬%%%%%%%%%%%%%%%%%%%%%%%
				    self.linear.set_gradients(w=weight_gradients, b=bias_gradients)  # è®¾ç½®æ¢¯åº¦å€¼
				    
				    list.reverse(input_gradients)  # input_gradientsæ˜¯é€†åºçš„ï¼Œæœ€åè¾“å‡ºæ—¶éœ€è¦reverseä¸€ä¸‹
				    print("sum(weight_gradients) = {}".format(np.sum(weight_gradients)))
				    
				    # np.stackçš„ä½œç”¨æ˜¯å°†åˆ—è¡¨è½¬å˜æˆä¸€ä¸ªçŸ©é˜µ
				    return np.stack(input_gradients), h_gradient
				```